{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f34668c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, Dropout\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import sklearn.datasets as skds\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f600b401",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tech</td>\n",
       "      <td>tv future in the hands of viewers with home th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>worldcom boss  left books alone  former worldc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sport</td>\n",
       "      <td>tigers wary of farrell  gamble  leicester say ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sport</td>\n",
       "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>ocean s twelve raids box office ocean s twelve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2220</th>\n",
       "      <td>business</td>\n",
       "      <td>cars pull down us retail figures us retail sal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2221</th>\n",
       "      <td>politics</td>\n",
       "      <td>kilroy unveils immigration policy ex-chatshow ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2222</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>rem announce new glasgow concert us band rem h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2223</th>\n",
       "      <td>politics</td>\n",
       "      <td>how political squabbles snowball it s become c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2224</th>\n",
       "      <td>sport</td>\n",
       "      <td>souness delight at euro progress boss graeme s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2225 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           category                                               text\n",
       "0              tech  tv future in the hands of viewers with home th...\n",
       "1          business  worldcom boss  left books alone  former worldc...\n",
       "2             sport  tigers wary of farrell  gamble  leicester say ...\n",
       "3             sport  yeading face newcastle in fa cup premiership s...\n",
       "4     entertainment  ocean s twelve raids box office ocean s twelve...\n",
       "...             ...                                                ...\n",
       "2220       business  cars pull down us retail figures us retail sal...\n",
       "2221       politics  kilroy unveils immigration policy ex-chatshow ...\n",
       "2222  entertainment  rem announce new glasgow concert us band rem h...\n",
       "2223       politics  how political squabbles snowball it s become c...\n",
       "2224          sport  souness delight at euro progress boss graeme s...\n",
       "\n",
       "[2225 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('bbc-text.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31f8d3ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2225 1780\n"
     ]
    }
   ],
   "source": [
    "# lets take 80% data as training and remaining 20% for test.\n",
    "train_size = int(len(data) * .80)\n",
    "print(len(data),train_size) \n",
    "train_posts = data['text'][:train_size]\n",
    "train_tags = data['category'][:train_size]\n",
    "test_posts = data['text'][train_size:]\n",
    "test_tags = data['category'][train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b4d5d50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sport', 'tech', 'business', 'politics', 'entertainment'} 5\n",
      "(1780, 15000)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 1],\n",
       "       [1, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, 1, 0],\n",
       "       [1, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5 groups\n",
    "num_labels = len(set(data['category']))\n",
    "print(set(data['category']),len(set(data['category'])))\n",
    "vocab_size = 15000\n",
    "batch_size = 100\n",
    " \n",
    "# define Tokenizer with Vocab Size\n",
    "tokenizer = Tokenizer(num_words=vocab_size)\n",
    "tokenizer.fit_on_texts(train_posts)\n",
    "x_train = tokenizer.texts_to_matrix(train_posts, mode='tfidf')\n",
    "print(x_train.shape)\n",
    "x_test = tokenizer.texts_to_matrix(test_posts, mode='tfidf')\n",
    "encoder = LabelBinarizer()\n",
    "encoder.fit(train_tags)\n",
    "y_train = encoder.transform(train_tags)\n",
    "y_test = encoder.transform(test_tags)\n",
    "y_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6c4319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 512)               7680512   \n",
      "                                                                 \n",
      " activation (Activation)     (None, 512)               0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 256)               0         \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 128)               0         \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 5)                 645       \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 5)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7,845,381\n",
      "Trainable params: 7,845,381\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/90\n",
      "17/17 [==============================] - 3s 145ms/step - loss: 0.8230 - accuracy: 0.7072 - val_loss: 0.0910 - val_accuracy: 0.9719\n",
      "Epoch 2/90\n",
      "17/17 [==============================] - 2s 130ms/step - loss: 0.0503 - accuracy: 0.9856 - val_loss: 0.1145 - val_accuracy: 0.9607\n",
      "Epoch 3/90\n",
      "17/17 [==============================] - 2s 131ms/step - loss: 0.0261 - accuracy: 0.9906 - val_loss: 0.0906 - val_accuracy: 0.9663\n",
      "Epoch 4/90\n",
      "17/17 [==============================] - 2s 127ms/step - loss: 0.0226 - accuracy: 0.9963 - val_loss: 0.1474 - val_accuracy: 0.9775\n",
      "Epoch 5/90\n",
      "17/17 [==============================] - 2s 125ms/step - loss: 0.0146 - accuracy: 0.9938 - val_loss: 0.0972 - val_accuracy: 0.9775\n",
      "Epoch 6/90\n",
      "17/17 [==============================] - 2s 126ms/step - loss: 0.0135 - accuracy: 0.9981 - val_loss: 0.0808 - val_accuracy: 0.9775\n",
      "Epoch 7/90\n",
      "17/17 [==============================] - 2s 125ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.1063 - val_accuracy: 0.9663\n",
      "Epoch 8/90\n",
      "17/17 [==============================] - 2s 126ms/step - loss: 0.0018 - accuracy: 0.9994 - val_loss: 0.0919 - val_accuracy: 0.9663\n",
      "Epoch 9/90\n",
      "17/17 [==============================] - 2s 124ms/step - loss: 0.0017 - accuracy: 0.9994 - val_loss: 0.0819 - val_accuracy: 0.9831\n",
      "Epoch 10/90\n",
      "17/17 [==============================] - 2s 126ms/step - loss: 0.0053 - accuracy: 0.9981 - val_loss: 0.1032 - val_accuracy: 0.9719\n",
      "Epoch 11/90\n",
      "17/17 [==============================] - 2s 130ms/step - loss: 0.0014 - accuracy: 0.9994 - val_loss: 0.1075 - val_accuracy: 0.9719\n",
      "Epoch 12/90\n",
      "17/17 [==============================] - 2s 130ms/step - loss: 1.1190e-04 - accuracy: 1.0000 - val_loss: 0.0991 - val_accuracy: 0.9775\n",
      "Epoch 13/90\n",
      "17/17 [==============================] - 2s 133ms/step - loss: 7.0575e-04 - accuracy: 0.9994 - val_loss: 0.1131 - val_accuracy: 0.9719\n",
      "Epoch 14/90\n",
      "17/17 [==============================] - 2s 126ms/step - loss: 1.1862e-04 - accuracy: 1.0000 - val_loss: 0.1179 - val_accuracy: 0.9719\n",
      "Epoch 15/90\n",
      " 1/17 [>.............................] - ETA: 1s - loss: 3.2615e-04 - accuracy: 1.0000"
     ]
    }
   ],
   "source": [
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(vocab_size,))) #can have positive and negative values\n",
    "model.add(Activation('relu')) #relu is a rectifier. regulates negative values. \n",
    "model.add(Dropout(0.3)) #values >0.3 are allowed due to this function \n",
    "\n",
    "\n",
    "model.add(Dense(256))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Dense(num_labels)) #num_labels is the total number of classes\n",
    "model.add(Activation('softmax')) #softmax activation is done on the output layer. Obtains probabalitic values of different classes.\n",
    "\n",
    "model.summary()\n",
    " \n",
    "model.compile(loss='categorical_crossentropy', #categorical cross...obtains the loss for multiple classes (i.e. categories entertainment, politcs...). Need to use with Softmax act function\n",
    "              optimizer='adam', #adam is an optimization function to minimize the y' value in the loss function. A typical optimization function used in text classification.\n",
    "              metrics=['accuracy']) #in the absence of confusion matrix, we can just use accuracy as metric. \n",
    " \n",
    "history = model.fit(x_train, y_train,    #fitting the model with the training set (x-train), y-train(training set data's tags encoded) The annotated dataset. \n",
    "                    batch_size=batch_size,\n",
    "                    epochs=90,\n",
    "                    verbose=1, #to randomize the data, not too important for this case. \n",
    "                    validation_split=0.1) #split from the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8218bf89",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(x_test, y_test,\n",
    "                       batch_size=batch_size, verbose=1) #take same number of batches as taken suring training.\n",
    "\n",
    "print('Test accuracy:', score[1]) #score[1] for one dimensional data. All data must be one-dimensional prior to entry into the NN\n",
    "\n",
    "text_labels = encoder.classes_ #to check how we have encoded the class of our categories, in order to test\n",
    "\n",
    "print(\"These are categories\",text_labels)\n",
    "\n",
    "\n",
    "for i in range(5,20): #range can be any number suitable to the range of the dataset\n",
    "    prediction = model.predict(np.array([x_test[i]])) #changed news data to numpy array to fit the requirements of the predict function \n",
    "    predicted_label = text_labels[np.argmax(prediction[0])] #argmax\n",
    "    print(test_posts.iloc[i])\n",
    "    print('Actual label:' + test_tags.iloc[i])\n",
    "    print(\"Predicted label: \" + predicted_label)\n",
    "    print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a226a048",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('news.txt',encoding=\"utf8\")\n",
    "c=file.read()\n",
    "print(c)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132d3f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    " \n",
    "labels = np.array(['business', 'entertainment' ,'politics' ,'sport' ,'tech']) #odrer is important \n",
    " \n",
    "test_files = ['news.txt']\n",
    "x_data = []\n",
    "for t_f in test_files:\n",
    "    t_f_data = Path(t_f).read_text()\n",
    "    print()\n",
    "    x_data.append(t_f_data)\n",
    "    print(x_data)\n",
    "x_data_series = pd.Series(x_data)\n",
    "x_tokenized = tokenizer.texts_to_matrix(x_data_series, mode='tfidf')\n",
    " \n",
    "i=0\n",
    "for x_t in x_tokenized:\n",
    "    prediction = model.predict(np.array([x_t]))\n",
    "    predicted_label = labels[np.argmax(prediction[0])]\n",
    "    print(\"File ->\", test_files[i], \"Predicted label: \" + predicted_label)\n",
    "\n",
    "print(\"Prediction in percentage = \\n\",prediction*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd0835d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction Visualization\n",
    "import plotly.graph_objects as go\n",
    "classes=labels.tolist()\n",
    "pred = (prediction*100).tolist()[0]\n",
    "fig = go.Figure([go.Bar(x=classes, y=pred)])\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b31dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ff6037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification report\n",
    "y_test_arg=np.argmax(y_test,axis=1)\n",
    "Y_pred = np.argmax(model.predict(x_test),axis=1)\n",
    "print('Confusion Report')\n",
    "cm = classification_report(y_test_arg, Y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef60164d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
